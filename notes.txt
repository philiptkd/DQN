27/4/19
Doing the "soft" update to the target network after every step took too much time. Switching to periodic hard updates made everything a lot faster. The agent learns quickly but then seems to get worse before getting better. I think this is because successful episodes correspond to less experience for this task. So most experience in the replay buffer is from poor episodes. Prioritized experience replay should address this problem.

28/4/19
Rank-based priority replay was a priority queue as a binary heap array. The heap array was not kept sorted. Sorting was done every million steps to save time and approximate a sorted array. The cumulative sampling probability distribution is approximated as a piecewise linear function with batch_size segments of equal probability. "At runtime, we sample a segment, and then sample uniformly among the transitions within it."

30/4/19
I'm unsure how to implement the rank-based prioritized replay. A max heap isn't as well-suited to representing a distribution as a sum-tree, seemingly. I should probably do the sampling separately, not as part of tree traversal. Then, once I have a rank to search for, I can find it in O(rank) time, due to Frederickson. Worst-case search time is linear in the size of the replay buffer. But worst-case is unlikely. An easier to implement way of finding the kth largest element takes O(k*log(k)) time.
...
The nodes of the tree (or whatever structure) will be sorted by TD-error. I'll need to store an index into the replay buffer in each node, too. This wasn't necessary in the sum-tree because its leaf indices were the same as the buffer indices.  
...
In order to get ranges of equal probability, I need to work with the zipf distribution. Scipy only has the zeta distribution misnamed as the zipf distribution. The difference is that zipf has support 1,2,...,N and zeta has support 1,2,... This is a problem, since the p-series diverges for powers <= 1, and we want alpha=0.6. I'll have to write it myself. As noted in the paper, though, this won't hurt RL runtime since these values can be precomputed for fixed values of N, alpha, and batch_size. However, I don't want to precompute and save these ranges of equal probability for every integer buffer size up to N. My compromise will be to precompute them for a few values of N. To sample from a buffer of a different size, I'll sample from it as if it's the next smallest size I have precomputed ranges for. This will bias my sampling towards trajectories with high TD-error, and that's ok. It will only happen while the replay buffer has yet to reach its final size, anyway.

1/5/19
The rank-based sampling version is very slow. It looks like most time is being spent finding the kth largest TD-error in the heap. I might have to look at the O(k) algorithm by Frederickson after all. Even then, though, it would still be slower than the O(log k) proportional sampling. Not to mention the periodic sorting that needs to be done in the rank-based solution. We'll see if it's fast enough to at least finish a small experiment.
...
I've thought of some obvious ways to speed up the current sampling algorithm. get_kth_largest is called batch_size times every step, but it could only be called one time--with the largest k--if the output were a sorted array of the k largest elements. This could be done with no extra time per call, and it would reduce runtime by a factor of batch_size. Also, indices into the heap for different values of k can be cached and reused before the heap is sorted again. This speed increase would come at the cost of using an unsorted heap for sampling, but the paper recommends that anyway.

2/5/19
After further consideration, just doing a full heap sort and saving the order is the best way to go if there will be multiple steps in between sortings. 
