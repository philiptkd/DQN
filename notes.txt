27/4/19
Doing the "soft" update to the target network after every step took too much time. Switching to periodic hard updates made everything a lot faster. The agent learns quickly but then seems to get worse before getting better. I think this is because successful episodes correspond to less experience for this task. So most experience in the replay buffer is from poor episodes. Prioritized experience replay should address this problem.

28/4/19
Rank-based priority replay was a priority queue as a binary heap array. The heap array was not kept sorted. Sorting was done every million steps to save time and approximate a sorted array. The cumulative sampling probability distribution is approximated as a piecewise linear function with batch_size segments of equal probability. "At runtime, we sample a segment, and then sample uniformly among the transitions within it."
